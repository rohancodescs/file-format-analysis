{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbd1900",
   "metadata": {},
   "source": [
    "#### Rohan Bhatt, Shubhang Srikoti \n",
    "##### MSML605 -  Investigating the Impact of Storage Formats\n",
    "Problem statement: How does the choice of storage format (CSV, Parquet, HDF5) impact the overall performance of a machine learning pipeline and its processes (data ingestion, memory overhead, time-to-train, and more).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4140169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/jtbontinck/amex-parquet-file?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.99G/8.99G [06:21<00:00, 25.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/rohan/.cache/kagglehub/datasets/jtbontinck/amex-parquet-file/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"jtbontinck/amex-parquet-file\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f15d0980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #THIS SCRIPT ACTUALLY ENDED UP INCREASING MY FILE SIZE to 12GB WHICH I USED GOING FORWARD\n",
    "\n",
    "# #tried script to convert 10gb parquet to 2gb parquet\n",
    "# import pyarrow.parquet as pq, pyarrow as pa, math\n",
    "# from pathlib import Path\n",
    "\n",
    "# SRC = Path(\"data.parquet\")          # 16-GB file\n",
    "# DST = Path(\"data_2gb.parquet\")\n",
    "\n",
    "# pq_src   = pq.ParquetFile(SRC)\n",
    "# n_rg     = pq_src.num_row_groups\n",
    "\n",
    "# # gather row-group sizes (compressed bytes on disk)\n",
    "# rg_sizes = [pq_src.metadata.row_group(i).total_byte_size for i in range(n_rg)]\n",
    "\n",
    "# target_bytes = 2 * 1024**3          # 2 GB\n",
    "# keep_rg      = []\n",
    "# cum          = 0\n",
    "# for i, sz in enumerate(rg_sizes):\n",
    "#     if cum + sz > target_bytes:\n",
    "#         break\n",
    "#     keep_rg.append(i)\n",
    "#     cum += sz\n",
    "\n",
    "# print(f\"Keeping {len(keep_rg)} row-groups  →  ~{cum/1024**3:.2f} GB\")\n",
    "\n",
    "# # read & write subset\n",
    "# tables = [pq_src.read_row_group(i) for i in keep_rg]\n",
    "# subset = pa.concat_tables(tables)\n",
    "# pq.write_table(subset, DST, compression=\"snappy\")   # or \"zstd\"\n",
    "\n",
    "# print(f\"Subset rows: {subset.num_rows:,}\")\n",
    "# print(\"Wrote:\", DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38dfe004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in file: 16895213\n",
      "Columns in file: 193\n",
      "Schema: <pyarrow._parquet.ParquetSchema object at 0x112aa7ac0>\n",
      "required group field_id=-1 duckdb_schema {\n",
      "  optional fixed_len_byte_array(16) field_id=-1 line_ID (UUID);\n",
      "  optional binary field_id=-1 customer_ID (String);\n",
      "  optional int64 field_id=-1 date (Timestamp(isAdjustedToUTC=false, timeUnit=microseconds, is_from_converted_type=false, force_set_converted_type=false));\n",
      "  optional float field_id=-1 P_2;\n",
      "  optional float field_id=-1 D_39;\n",
      "  optional float field_id=-1 B_1;\n",
      "  optional float field_id=-1 B_2;\n",
      "  optional float field_id=-1 R_1;\n",
      "  optional float field_id=-1 S_3;\n",
      "  optional float field_id=-1 D_41;\n",
      "  optional float field_id=-1 B_3;\n",
      "  optional float field_id=-1 D_42;\n",
      "  optional float field_id=-1 D_43;\n",
      "  optional float field_id=-1 D_44;\n",
      "  optional float field_id=-1 B_4;\n",
      "  optional float field_id=-1 D_45;\n",
      "  optional float field_id=-1 B_5;\n",
      "  optional float field_id=-1 R_2;\n",
      "  optional float field_id=-1 D_46;\n",
      "  optional float field_id=-1 D_47;\n",
      "  optional float field_id=-1 D_48;\n",
      "  optional float field_id=-1 D_49;\n",
      "  optional float field_id=-1 B_6;\n",
      "  optional float field_id=-1 B_7;\n",
      "  optional float field_id=-1 B_8;\n",
      "  optional float field_id=-1 D_50;\n",
      "  optional float field_id=-1 D_51;\n",
      "  optional float field_id=-1 B_9;\n",
      "  optional float field_id=-1 R_3;\n",
      "  optional float field_id=-1 D_52;\n",
      "  optional float field_id=-1 P_3;\n",
      "  optional float field_id=-1 B_10;\n",
      "  optional float field_id=-1 D_53;\n",
      "  optional float field_id=-1 S_5;\n",
      "  optional float field_id=-1 B_11;\n",
      "  optional float field_id=-1 S_6;\n",
      "  optional float field_id=-1 D_54;\n",
      "  optional float field_id=-1 R_4;\n",
      "  optional float field_id=-1 S_7;\n",
      "  optional float field_id=-1 B_12;\n",
      "  optional float field_id=-1 S_8;\n",
      "  optional float field_id=-1 D_55;\n",
      "  optional float field_id=-1 D_56;\n",
      "  optional float field_id=-1 B_13;\n",
      "  optional float field_id=-1 R_5;\n",
      "  optional float field_id=-1 D_58;\n",
      "  optional float field_id=-1 S_9;\n",
      "  optional float field_id=-1 B_14;\n",
      "  optional float field_id=-1 D_59;\n",
      "  optional float field_id=-1 D_60;\n",
      "  optional float field_id=-1 D_61;\n",
      "  optional float field_id=-1 B_15;\n",
      "  optional float field_id=-1 S_11;\n",
      "  optional float field_id=-1 D_62;\n",
      "  optional binary field_id=-1 D_63 (String);\n",
      "  optional binary field_id=-1 D_64 (String);\n",
      "  optional float field_id=-1 D_65;\n",
      "  optional float field_id=-1 B_16;\n",
      "  optional float field_id=-1 B_17;\n",
      "  optional float field_id=-1 B_18;\n",
      "  optional float field_id=-1 B_19;\n",
      "  optional float field_id=-1 D_66;\n",
      "  optional float field_id=-1 B_20;\n",
      "  optional float field_id=-1 D_68;\n",
      "  optional float field_id=-1 S_12;\n",
      "  optional float field_id=-1 R_6;\n",
      "  optional float field_id=-1 S_13;\n",
      "  optional float field_id=-1 B_21;\n",
      "  optional float field_id=-1 D_69;\n",
      "  optional float field_id=-1 B_22;\n",
      "  optional float field_id=-1 D_70;\n",
      "  optional float field_id=-1 D_71;\n",
      "  optional float field_id=-1 D_72;\n",
      "  optional float field_id=-1 S_15;\n",
      "  optional float field_id=-1 B_23;\n",
      "  optional float field_id=-1 D_73;\n",
      "  optional float field_id=-1 P_4;\n",
      "  optional float field_id=-1 D_74;\n",
      "  optional float field_id=-1 D_75;\n",
      "  optional float field_id=-1 D_76;\n",
      "  optional float field_id=-1 B_24;\n",
      "  optional float field_id=-1 R_7;\n",
      "  optional float field_id=-1 D_77;\n",
      "  optional float field_id=-1 B_25;\n",
      "  optional float field_id=-1 B_26;\n",
      "  optional float field_id=-1 D_78;\n",
      "  optional float field_id=-1 D_79;\n",
      "  optional float field_id=-1 R_8;\n",
      "  optional float field_id=-1 R_9;\n",
      "  optional float field_id=-1 S_16;\n",
      "  optional float field_id=-1 D_80;\n",
      "  optional float field_id=-1 R_10;\n",
      "  optional float field_id=-1 R_11;\n",
      "  optional float field_id=-1 B_27;\n",
      "  optional float field_id=-1 D_81;\n",
      "  optional float field_id=-1 D_82;\n",
      "  optional float field_id=-1 S_17;\n",
      "  optional float field_id=-1 R_12;\n",
      "  optional float field_id=-1 B_28;\n",
      "  optional float field_id=-1 R_13;\n",
      "  optional float field_id=-1 D_83;\n",
      "  optional float field_id=-1 R_14;\n",
      "  optional float field_id=-1 R_15;\n",
      "  optional float field_id=-1 D_84;\n",
      "  optional float field_id=-1 R_16;\n",
      "  optional float field_id=-1 B_29;\n",
      "  optional float field_id=-1 B_30;\n",
      "  optional float field_id=-1 S_18;\n",
      "  optional float field_id=-1 D_86;\n",
      "  optional float field_id=-1 D_87;\n",
      "  optional float field_id=-1 R_17;\n",
      "  optional float field_id=-1 R_18;\n",
      "  optional float field_id=-1 D_88;\n",
      "  optional int64 field_id=-1 B_31 (Int(bitWidth=64, isSigned=true));\n",
      "  optional float field_id=-1 S_19;\n",
      "  optional float field_id=-1 R_19;\n",
      "  optional float field_id=-1 B_32;\n",
      "  optional float field_id=-1 S_20;\n",
      "  optional float field_id=-1 R_20;\n",
      "  optional float field_id=-1 R_21;\n",
      "  optional float field_id=-1 B_33;\n",
      "  optional float field_id=-1 D_89;\n",
      "  optional float field_id=-1 R_22;\n",
      "  optional float field_id=-1 R_23;\n",
      "  optional float field_id=-1 D_91;\n",
      "  optional float field_id=-1 D_92;\n",
      "  optional float field_id=-1 D_93;\n",
      "  optional float field_id=-1 D_94;\n",
      "  optional float field_id=-1 R_24;\n",
      "  optional float field_id=-1 R_25;\n",
      "  optional float field_id=-1 D_96;\n",
      "  optional float field_id=-1 S_22;\n",
      "  optional float field_id=-1 S_23;\n",
      "  optional float field_id=-1 S_24;\n",
      "  optional float field_id=-1 S_25;\n",
      "  optional float field_id=-1 S_26;\n",
      "  optional float field_id=-1 D_102;\n",
      "  optional float field_id=-1 D_103;\n",
      "  optional float field_id=-1 D_104;\n",
      "  optional float field_id=-1 D_105;\n",
      "  optional float field_id=-1 D_106;\n",
      "  optional float field_id=-1 D_107;\n",
      "  optional float field_id=-1 B_36;\n",
      "  optional float field_id=-1 B_37;\n",
      "  optional float field_id=-1 R_26;\n",
      "  optional float field_id=-1 R_27;\n",
      "  optional float field_id=-1 B_38;\n",
      "  optional float field_id=-1 D_108;\n",
      "  optional float field_id=-1 D_109;\n",
      "  optional float field_id=-1 D_110;\n",
      "  optional float field_id=-1 D_111;\n",
      "  optional float field_id=-1 B_39;\n",
      "  optional float field_id=-1 D_112;\n",
      "  optional float field_id=-1 B_40;\n",
      "  optional float field_id=-1 S_27;\n",
      "  optional float field_id=-1 D_113;\n",
      "  optional float field_id=-1 D_114;\n",
      "  optional float field_id=-1 D_115;\n",
      "  optional float field_id=-1 D_116;\n",
      "  optional float field_id=-1 D_117;\n",
      "  optional float field_id=-1 D_118;\n",
      "  optional float field_id=-1 D_119;\n",
      "  optional float field_id=-1 D_120;\n",
      "  optional float field_id=-1 D_121;\n",
      "  optional float field_id=-1 D_122;\n",
      "  optional float field_id=-1 D_123;\n",
      "  optional float field_id=-1 D_124;\n",
      "  optional float field_id=-1 D_125;\n",
      "  optional float field_id=-1 D_126;\n",
      "  optional float field_id=-1 D_127;\n",
      "  optional float field_id=-1 D_128;\n",
      "  optional float field_id=-1 D_129;\n",
      "  optional float field_id=-1 B_41;\n",
      "  optional float field_id=-1 B_42;\n",
      "  optional float field_id=-1 D_130;\n",
      "  optional float field_id=-1 D_131;\n",
      "  optional float field_id=-1 D_132;\n",
      "  optional float field_id=-1 D_133;\n",
      "  optional float field_id=-1 R_28;\n",
      "  optional float field_id=-1 D_134;\n",
      "  optional float field_id=-1 D_135;\n",
      "  optional float field_id=-1 D_136;\n",
      "  optional float field_id=-1 D_137;\n",
      "  optional float field_id=-1 D_138;\n",
      "  optional float field_id=-1 D_139;\n",
      "  optional float field_id=-1 D_140;\n",
      "  optional float field_id=-1 D_141;\n",
      "  optional float field_id=-1 D_142;\n",
      "  optional float field_id=-1 D_143;\n",
      "  optional float field_id=-1 D_144;\n",
      "  optional float field_id=-1 D_145;\n",
      "  optional int32 field_id=-1 target (Int(bitWidth=8, isSigned=true));\n",
      "  optional int32 field_id=-1 test (Int(bitWidth=8, isSigned=true));\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "#sanity check of parquet file\n",
    "pq_file = pq.ParquetFile(\"data.parquet\")\n",
    "print(\"Rows in file:\", pq_file.metadata.num_rows)\n",
    "print(\"Columns in file:\", pq_file.metadata.num_columns)\n",
    "print(\"Schema:\", pq_file.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import time, datetime, os, psutil\n",
    "import xgboost as xgb\n",
    "from pathlib import Path, PureWindowsPath\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc8fc3",
   "metadata": {},
   "source": [
    "Below is converting the initial ~11gb parquet file to CSV. To do this, I set the source (initial parquet file) and the destination path, and opening the parquet to discover how many row-groups to iterate. Then the loop reads one row-group at a time to keep memory low so the entire process doesn't explode in memory, then converting the columnar arrow buffers into a pandas dataframe, appending that chunk to the CSV (writing the header only once), then freeing the RAM before loading the next chunk. Throughout this process I monitored the conversion, and although I forgot to code the time it took ~25 min to convert ending in ~32 gb CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a30a1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row groups in file: 169\n",
      "row-group 1/169 appended\n",
      "row-group 2/169 appended\n",
      "row-group 3/169 appended\n",
      "row-group 4/169 appended\n",
      "row-group 5/169 appended\n",
      "row-group 6/169 appended\n",
      "row-group 7/169 appended\n",
      "row-group 8/169 appended\n",
      "row-group 9/169 appended\n",
      "row-group 10/169 appended\n",
      "row-group 11/169 appended\n",
      "row-group 12/169 appended\n",
      "row-group 13/169 appended\n",
      "row-group 14/169 appended\n",
      "row-group 15/169 appended\n",
      "row-group 16/169 appended\n",
      "row-group 17/169 appended\n",
      "row-group 18/169 appended\n",
      "row-group 19/169 appended\n",
      "row-group 20/169 appended\n",
      "row-group 21/169 appended\n",
      "row-group 22/169 appended\n",
      "row-group 23/169 appended\n",
      "row-group 24/169 appended\n",
      "row-group 25/169 appended\n",
      "row-group 26/169 appended\n",
      "row-group 27/169 appended\n",
      "row-group 28/169 appended\n",
      "row-group 29/169 appended\n",
      "row-group 30/169 appended\n",
      "row-group 31/169 appended\n",
      "row-group 32/169 appended\n",
      "row-group 33/169 appended\n",
      "row-group 34/169 appended\n",
      "row-group 35/169 appended\n",
      "row-group 36/169 appended\n",
      "row-group 37/169 appended\n",
      "row-group 38/169 appended\n",
      "row-group 39/169 appended\n",
      "row-group 40/169 appended\n",
      "row-group 41/169 appended\n",
      "row-group 42/169 appended\n",
      "row-group 43/169 appended\n",
      "row-group 44/169 appended\n",
      "row-group 45/169 appended\n",
      "row-group 46/169 appended\n",
      "row-group 47/169 appended\n",
      "row-group 48/169 appended\n",
      "row-group 49/169 appended\n",
      "row-group 50/169 appended\n",
      "row-group 51/169 appended\n",
      "row-group 52/169 appended\n",
      "row-group 53/169 appended\n",
      "row-group 54/169 appended\n",
      "row-group 55/169 appended\n",
      "row-group 56/169 appended\n",
      "row-group 57/169 appended\n",
      "row-group 58/169 appended\n",
      "row-group 59/169 appended\n",
      "row-group 60/169 appended\n",
      "row-group 61/169 appended\n",
      "row-group 62/169 appended\n",
      "row-group 63/169 appended\n",
      "row-group 64/169 appended\n",
      "row-group 65/169 appended\n",
      "row-group 66/169 appended\n",
      "row-group 67/169 appended\n",
      "row-group 68/169 appended\n",
      "row-group 69/169 appended\n",
      "row-group 70/169 appended\n",
      "row-group 71/169 appended\n",
      "row-group 72/169 appended\n",
      "row-group 73/169 appended\n",
      "row-group 74/169 appended\n",
      "row-group 75/169 appended\n",
      "row-group 76/169 appended\n",
      "row-group 77/169 appended\n",
      "row-group 78/169 appended\n",
      "row-group 79/169 appended\n",
      "row-group 80/169 appended\n",
      "row-group 81/169 appended\n",
      "row-group 82/169 appended\n",
      "row-group 83/169 appended\n",
      "row-group 84/169 appended\n",
      "row-group 85/169 appended\n",
      "row-group 86/169 appended\n",
      "row-group 87/169 appended\n",
      "row-group 88/169 appended\n",
      "row-group 89/169 appended\n",
      "row-group 90/169 appended\n",
      "row-group 91/169 appended\n",
      "row-group 92/169 appended\n",
      "row-group 93/169 appended\n",
      "row-group 94/169 appended\n",
      "row-group 95/169 appended\n",
      "row-group 96/169 appended\n",
      "row-group 97/169 appended\n",
      "row-group 98/169 appended\n",
      "row-group 99/169 appended\n",
      "row-group 100/169 appended\n",
      "row-group 101/169 appended\n",
      "row-group 102/169 appended\n",
      "row-group 103/169 appended\n",
      "row-group 104/169 appended\n",
      "row-group 105/169 appended\n",
      "row-group 106/169 appended\n",
      "row-group 107/169 appended\n",
      "row-group 108/169 appended\n",
      "row-group 109/169 appended\n",
      "row-group 110/169 appended\n",
      "row-group 111/169 appended\n",
      "row-group 112/169 appended\n",
      "row-group 113/169 appended\n",
      "row-group 114/169 appended\n",
      "row-group 115/169 appended\n",
      "row-group 116/169 appended\n",
      "row-group 117/169 appended\n",
      "row-group 118/169 appended\n",
      "row-group 119/169 appended\n",
      "row-group 120/169 appended\n",
      "row-group 121/169 appended\n",
      "row-group 122/169 appended\n",
      "row-group 123/169 appended\n",
      "row-group 124/169 appended\n",
      "row-group 125/169 appended\n",
      "row-group 126/169 appended\n",
      "row-group 127/169 appended\n",
      "row-group 128/169 appended\n",
      "row-group 129/169 appended\n",
      "row-group 130/169 appended\n",
      "row-group 131/169 appended\n",
      "row-group 132/169 appended\n",
      "row-group 133/169 appended\n",
      "row-group 134/169 appended\n",
      "row-group 135/169 appended\n",
      "row-group 136/169 appended\n",
      "row-group 137/169 appended\n",
      "row-group 138/169 appended\n",
      "row-group 139/169 appended\n",
      "row-group 140/169 appended\n",
      "row-group 141/169 appended\n",
      "row-group 142/169 appended\n",
      "row-group 143/169 appended\n",
      "row-group 144/169 appended\n",
      "row-group 145/169 appended\n",
      "row-group 146/169 appended\n",
      "row-group 147/169 appended\n",
      "row-group 148/169 appended\n",
      "row-group 149/169 appended\n",
      "row-group 150/169 appended\n",
      "row-group 151/169 appended\n",
      "row-group 152/169 appended\n",
      "row-group 153/169 appended\n",
      "row-group 154/169 appended\n",
      "row-group 155/169 appended\n",
      "row-group 156/169 appended\n",
      "row-group 157/169 appended\n",
      "row-group 158/169 appended\n",
      "row-group 159/169 appended\n",
      "row-group 160/169 appended\n",
      "row-group 161/169 appended\n",
      "row-group 162/169 appended\n",
      "row-group 163/169 appended\n",
      "row-group 164/169 appended\n",
      "row-group 165/169 appended\n",
      "row-group 166/169 appended\n",
      "row-group 167/169 appended\n",
      "row-group 168/169 appended\n",
      "row-group 169/169 appended\n",
      "All done -> data.csv\n"
     ]
    }
   ],
   "source": [
    "# in/out file paths\n",
    "IN_FILE = Path(\"data.parquet\")\n",
    "OUT_CSV = Path(\"data.csv\") \n",
    "#opening the parquet file\n",
    "pq_file = pq.ParquetFile(IN_FILE, memory_map=True) #zero copy i/o data\n",
    "num_row_groups = pq_file.num_row_groups #number of row groups, meaning the partitions on disk\n",
    "print(f\"Row groups in file: {num_row_groups}\")\n",
    "\n",
    "# write loop\n",
    "first_chunk = True #controlling the header write\n",
    "for row_group in range(num_row_groups):\n",
    "    # load one row group into an arrow table to stay off heap but still columnar\n",
    "    table = pq_file.read_row_group(row_group)\n",
    "    # converting to pandas, the arrowDtype preserves the nullable types during the conversion\n",
    "    df = table.to_pandas(types_mapper=pd.ArrowDtype)\n",
    "    # writing / appending\n",
    "    if first_chunk: #create/overwrite file and write to header row\n",
    "        df.to_csv(OUT_CSV, index=False, mode=\"w\", header=True)\n",
    "        first_chunk = False\n",
    "    else: #append rows and skip header\n",
    "        df.to_csv(OUT_CSV, index=False, mode=\"a\", header=False)\n",
    "    \n",
    "    # free memory for chunk before next iteration\n",
    "    del df, table\n",
    "    gc.collect()\n",
    "    print(f\"row-group {row_group+1}/{num_row_groups} appended\")\n",
    "\n",
    "print(\"All done ->\", OUT_CSV) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fd0c8",
   "metadata": {},
   "source": [
    "Before converting to HDF5, below is a sanity check that there wasn't any loss or misrepresentation in data between the files. The only visible difference is the date display; Parquet keeps it as pandas datetime64[ns], while the csv reader shows a string timestamp of object dtype. we can cast this later but data is still preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66e6f3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             line_ID  \\\n",
      "0  b'\\xb6a\\x82\\x86f#F\\x1d\\x8c\\x94\\x7f\\x8d\\x944\\xd...   \n",
      "1        b'L\\xa8+-\\xa8\\x8dM\\xa9\\x96g\\xed0I\\x95\\x1e$'   \n",
      "2        b']s_\\x87\\xaf B\\xec\\xbeEg\\xb5\\x1e\\xb2\\xaed'   \n",
      "3     b'\\xfb^\\xd4{Q\\xb5HO\\xa8\\xb6\\xf6\\xca\\xb1]@\\x99'   \n",
      "4  b'`\\xa5\\x96\\xf6\\x1b\\rG\\x8d\\xab\\\\\\x16\\x8d\\xe1\\x...   \n",
      "\n",
      "                                         customer_ID       date       P_2  \\\n",
      "0  d00b98b2401d26197fa1d6102cdc1c9bbed7c066b8aaa9... 2018-03-06  0.366254   \n",
      "1  d00bc5e66e3aac9eae7c9e94621b36d196566d61ef7a32... 2018-03-25  0.312623   \n",
      "2  d00bd125cf6fa463a6c57b9959b8a4197f6f79fb154fee... 2018-03-28  0.395606   \n",
      "3  d00bfbdee3081206258a4b4fb2ef2eb311697f37056bfb... 2018-03-01  0.977543   \n",
      "4  d00c0dd295ada176c4e697d4cc1cd2f0d572870f770859... 2018-03-26  0.934237   \n",
      "\n",
      "       D_39       B_1       B_2       R_1       S_3      D_41  ...     D_138  \\\n",
      "0  0.003860  0.009151  0.818901  0.008979  0.143153  0.005497  ...  0.500092   \n",
      "1  0.179014  0.560108  0.029272  0.756391  0.091940  0.005489  ...       NaN   \n",
      "2  1.066026  0.731072  0.019496  0.751631  0.728473  0.574862  ...       NaN   \n",
      "3  0.299848  0.016523  1.002691  0.009632  0.107092  0.003081  ...       NaN   \n",
      "4  0.003405  0.295735  1.001624  0.005357  0.142128  0.000591  ...       NaN   \n",
      "\n",
      "      D_139     D_140     D_141     D_142     D_143     D_144     D_145  \\\n",
      "0  0.004264  0.009833  0.000985       NaN  0.006703  0.005501  0.005352   \n",
      "1  0.005151  0.005813  0.007565       NaN  0.000990  0.001063  0.008251   \n",
      "2  1.008811  0.001852  0.964900  0.478015  1.001196  0.173963  0.091996   \n",
      "3  0.006636  0.003483  0.005275       NaN  0.009845  0.009467  0.008976   \n",
      "4  1.004110  0.002928  1.086800  1.076618  1.001960  1.003030  0.097089   \n",
      "\n",
      "   target  test  \n",
      "0     0.0     0  \n",
      "1     1.0     0  \n",
      "2     1.0     0  \n",
      "3     0.0     0  \n",
      "4     0.0     0  \n",
      "\n",
      "[5 rows x 193 columns]\n",
      "-------\n",
      "-------\n",
      "-------\n",
      "-------\n",
      "-------\n",
      "                                             line_ID  \\\n",
      "0  b'\\xb6a\\x82\\x86f#F\\x1d\\x8c\\x94\\x7f\\x8d\\x944\\xd...   \n",
      "1        b'L\\xa8+-\\xa8\\x8dM\\xa9\\x96g\\xed0I\\x95\\x1e$'   \n",
      "2        b']s_\\x87\\xaf B\\xec\\xbeEg\\xb5\\x1e\\xb2\\xaed'   \n",
      "3     b'\\xfb^\\xd4{Q\\xb5HO\\xa8\\xb6\\xf6\\xca\\xb1]@\\x99'   \n",
      "4  b'`\\xa5\\x96\\xf6\\x1b\\rG\\x8d\\xab\\\\\\x16\\x8d\\xe1\\x...   \n",
      "\n",
      "                                         customer_ID                 date  \\\n",
      "0  d00b98b2401d26197fa1d6102cdc1c9bbed7c066b8aaa9...  2018-03-06 00:00:00   \n",
      "1  d00bc5e66e3aac9eae7c9e94621b36d196566d61ef7a32...  2018-03-25 00:00:00   \n",
      "2  d00bd125cf6fa463a6c57b9959b8a4197f6f79fb154fee...  2018-03-28 00:00:00   \n",
      "3  d00bfbdee3081206258a4b4fb2ef2eb311697f37056bfb...  2018-03-01 00:00:00   \n",
      "4  d00c0dd295ada176c4e697d4cc1cd2f0d572870f770859...  2018-03-26 00:00:00   \n",
      "\n",
      "        P_2      D_39       B_1       B_2       R_1       S_3      D_41  ...  \\\n",
      "0  0.366254  0.003860  0.009151  0.818901  0.008979  0.143153  0.005497  ...   \n",
      "1  0.312623  0.179014  0.560108  0.029272  0.756391  0.091940  0.005489  ...   \n",
      "2  0.395606  1.066026  0.731072  0.019496  0.751631  0.728473  0.574862  ...   \n",
      "3  0.977543  0.299848  0.016523  1.002691  0.009632  0.107092  0.003081  ...   \n",
      "4  0.934237  0.003405  0.295735  1.001624  0.005357  0.142128  0.000591  ...   \n",
      "\n",
      "      D_138     D_139     D_140     D_141     D_142     D_143     D_144  \\\n",
      "0  0.500092  0.004264  0.009833  0.000985       NaN  0.006703  0.005501   \n",
      "1       NaN  0.005151  0.005813  0.007565       NaN  0.000990  0.001063   \n",
      "2       NaN  1.008811  0.001852  0.964900  0.478015  1.001196  0.173963   \n",
      "3       NaN  0.006636  0.003483  0.005275       NaN  0.009845  0.009467   \n",
      "4       NaN  1.004110  0.002928  1.086800  1.076618  1.001960  1.003030   \n",
      "\n",
      "      D_145  target  test  \n",
      "0  0.005352       0     0  \n",
      "1  0.008251       1     0  \n",
      "2  0.091996       1     0  \n",
      "3  0.008976       0     0  \n",
      "4  0.097089       0     0  \n",
      "\n",
      "[5 rows x 193 columns]\n",
      "row lengths are the same: True\n"
     ]
    }
   ],
   "source": [
    "#verifying both files before hdf5 conversion\n",
    "df_pq = pq.read_table(\"data.parquet\").to_pandas().head(5)\n",
    "df_csv = pd.read_csv(\"data.csv\", nrows=5)\n",
    "\n",
    "print(df_pq.head())\n",
    "print(\"-------\\n-------\\n-------\\n-------\\n-------\")\n",
    "print(df_csv.head())\n",
    "\n",
    "print(\"row lengths are the same:\", len(df_pq) == len(df_csv)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6434c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy : 1.24.4\n",
      "PyTables : 3.9.2\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tables\n",
    "print(\"NumPy :\", numpy.__version__)\n",
    "print(\"PyTables :\", tables.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7494d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ row-group 1/17  |  elapsed 0.4 min\n",
      "✓ row-group 2/17  |  elapsed 2.7 min\n",
      "✓ row-group 3/17  |  elapsed 5.4 min\n",
      "✓ row-group 4/17  |  elapsed 7.6 min\n",
      "✓ row-group 5/17  |  elapsed 13.1 min\n",
      "✓ row-group 6/17  |  elapsed 71.3 min\n",
      "✓ row-group 7/17  |  elapsed 73.4 min\n",
      "✓ row-group 8/17  |  elapsed 75.6 min\n",
      "✓ row-group 9/17  |  elapsed 77.8 min\n",
      "✓ row-group 10/17  |  elapsed 230.3 min\n",
      "✓ row-group 11/17  |  elapsed 335.3 min\n",
      "✓ row-group 12/17  |  elapsed 361.5 min\n",
      "✓ row-group 13/17  |  elapsed 363.7 min\n",
      "✓ row-group 14/17  |  elapsed 366.0 min\n",
      "✓ row-group 15/17  |  elapsed 368.3 min\n",
      "✓ row-group 16/17  |  elapsed 370.8 min\n",
      "✓ row-group 17/17  |  elapsed 373.0 min\n",
      "\n",
      "Parquet → HDF5 completed in 373.0 minutes\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq, pandas as pd, numpy as np, gc, time\n",
    "from pathlib import Path\n",
    "#source / destination files\n",
    "IN_PARQUET = Path(\"data.parquet\")\n",
    "OUT_H5 = Path(\"data.h5\")\n",
    "\n",
    "#open parquet file and getting how many row groups we need to stream\n",
    "pq_file = pq.ParquetFile(IN_PARQUET, memory_map=True) # zero copy i/o data\n",
    "num_row_groups = pq_file.num_row_groups\n",
    "\n",
    "#iterating through the row groups and writing to hdf5\n",
    "t0 = time.time()\n",
    "# \"w\" -> create / overwrite HDF5 file\n",
    "# \"zlib\" -> compression algorithm\n",
    "# \"6\" -> compression level (0-9), 1 being fastest but minimal compression, 9 being slowest and highest compression\n",
    "with pd.HDFStore(OUT_H5, \"w\", complib=\"zlib\", complevel=6) as s:\n",
    "    for i in range(num_row_groups):\n",
    "        #loading one row group into pandas df\n",
    "        df = pq_file.read_row_group(i).to_pandas()\n",
    "        # bytes → hex-strings\n",
    "        for col in df.select_dtypes(\"object\"):\n",
    "            if isinstance(df[col].iloc[0], (bytes, bytearray)):\n",
    "                df[col] = df[col].apply(lambda b: b.hex())\n",
    "\n",
    "        # force consistent NumPy int8 for label columns\n",
    "        for col in [\"target\", \"test\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(-1).astype(np.int8)\n",
    "\n",
    "        s.append(\"train\", df, data_columns=True, index=False)\n",
    "        del df; gc.collect()\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"✓ row-group {i+1}/{num_row_groups}  |  elapsed {elapsed/60:.1f} min\")\n",
    "\n",
    "total = time.time() - t0\n",
    "print(f\"\\nParquet → HDF5 completed in {total/60:.1f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40dec8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            line_ID  \\\n",
      "0  b66182866623461d8c947f8d9434d7b6   \n",
      "1  4ca82b2da88d4da99667ed3049951e24   \n",
      "2  5d735f87af2042ecbe4567b51eb2ae64   \n",
      "3  fb5ed47b51b5484fa8b6f6cab15d4099   \n",
      "4  60a596f61b0d478dab5c168de1c6f6be   \n",
      "\n",
      "                                         customer_ID                    date  \\\n",
      "0  d00b98b2401d26197fa1d6102cdc1c9bbed7c066b8aaa9... 1970-01-18 14:18:14.400   \n",
      "1  d00bc5e66e3aac9eae7c9e94621b36d196566d61ef7a32... 1970-01-18 14:45:36.000   \n",
      "2  d00bd125cf6fa463a6c57b9959b8a4197f6f79fb154fee... 1970-01-18 14:49:55.200   \n",
      "3  d00bfbdee3081206258a4b4fb2ef2eb311697f37056bfb... 1970-01-18 14:11:02.400   \n",
      "4  d00c0dd295ada176c4e697d4cc1cd2f0d572870f770859... 1970-01-18 14:47:02.400   \n",
      "\n",
      "        P_2      D_39       B_1       B_2       R_1       S_3      D_41  ...  \\\n",
      "0  0.366254  0.003860  0.009151  0.818901  0.008979  0.143153  0.005497  ...   \n",
      "1  0.312623  0.179014  0.560108  0.029272  0.756391  0.091940  0.005489  ...   \n",
      "2  0.395606  1.066026  0.731072  0.019496  0.751631  0.728473  0.574862  ...   \n",
      "3  0.977543  0.299848  0.016523  1.002691  0.009632  0.107092  0.003081  ...   \n",
      "4  0.934237  0.003405  0.295735  1.001624  0.005357  0.142128  0.000591  ...   \n",
      "\n",
      "      D_138     D_139     D_140     D_141     D_142     D_143     D_144  \\\n",
      "0  0.500092  0.004264  0.009833  0.000985       NaN  0.006703  0.005501   \n",
      "1       NaN  0.005151  0.005813  0.007565       NaN  0.000990  0.001063   \n",
      "2       NaN  1.008811  0.001852  0.964900  0.478015  1.001196  0.173963   \n",
      "3       NaN  0.006636  0.003483  0.005275       NaN  0.009845  0.009467   \n",
      "4       NaN  1.004110  0.002928  1.086800  1.076618  1.001960  1.003030   \n",
      "\n",
      "      D_145  target  test  \n",
      "0  0.005352       0     0  \n",
      "1  0.008251       1     0  \n",
      "2  0.091996       1     0  \n",
      "3  0.008976       0     0  \n",
      "4  0.097089       0     0  \n",
      "\n",
      "[5 rows x 193 columns]\n",
      "\n",
      "HDF5 quick read time: 0.07877683639526367 sec\n",
      "HDF5 size on disk: 8.900587243959308 GB\n"
     ]
    }
   ],
   "source": [
    "#sanity check of hdf5 file\n",
    "import pandas as pd, os, time\n",
    "\n",
    "start = time.time()\n",
    "df_head = pd.read_hdf(\"data_2gb.h5\", key=\"train\", stop=5)\n",
    "print(df_head.head())\n",
    "print(\"\\nHDF5 quick read time:\", time.time()-start, \"sec\")\n",
    "print(\"HDF5 size on disk:\", os.path.getsize(\"data_2gb.h5\")/1024**3, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc922ff",
   "metadata": {},
   "source": [
    "##### Actual benchmarks of each format\n",
    "(See run_benchmark.py, given the nature of how lengthy the process is I had to run each format separately, and asynch)\n",
    "\n",
    "| format  | rows     | load_sec | train_sec | peak_ram_gb |\n",
    "|---------|----------|----------|-----------|-------------|\n",
    "| Parquet | 1000000  | 22.19    | 7.83      | 0.92        |\n",
    "| CSV     | 1000000  | 375.81   | 7.76      | 3.84        |\n",
    "| HDF5    | 1000000  | 96.84    | 7.78      | 7.66        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d1fe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b94a523",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
