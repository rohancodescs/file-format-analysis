{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbd1900",
   "metadata": {},
   "source": [
    "#### Rohan Bhatt, Shubhang Srikoti \n",
    "##### MSML605 -  Investigating the Impact of Storage Formats\n",
    "Problem statement: How does the choice of storage format (CSV, Parquet, HDF5) impact the overall performance of a machine learning pipeline and its processes (data ingestion, memory overhead, time-to-train, and more).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4140169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/jtbontinck/amex-parquet-file?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.99G/8.99G [06:21<00:00, 25.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/rohan/.cache/kagglehub/datasets/jtbontinck/amex-parquet-file/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"jtbontinck/amex-parquet-file\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f15d0980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #THIS SCRIPT ACTUALLY ENDED UP INCREASING MY FILE SIZE to 12GB WHICH I USED GOING FORWARD\n",
    "\n",
    "# #tried script to convert 10gb parquet to 2gb parquet\n",
    "# import pyarrow.parquet as pq, pyarrow as pa, math\n",
    "# from pathlib import Path\n",
    "\n",
    "# SRC = Path(\"data.parquet\")          # 16-GB file\n",
    "# DST = Path(\"data_2gb.parquet\")\n",
    "\n",
    "# pq_src   = pq.ParquetFile(SRC)\n",
    "# n_rg     = pq_src.num_row_groups\n",
    "\n",
    "# # gather row-group sizes (compressed bytes on disk)\n",
    "# rg_sizes = [pq_src.metadata.row_group(i).total_byte_size for i in range(n_rg)]\n",
    "\n",
    "# target_bytes = 2 * 1024**3          # 2 GB\n",
    "# keep_rg      = []\n",
    "# cum          = 0\n",
    "# for i, sz in enumerate(rg_sizes):\n",
    "#     if cum + sz > target_bytes:\n",
    "#         break\n",
    "#     keep_rg.append(i)\n",
    "#     cum += sz\n",
    "\n",
    "# print(f\"Keeping {len(keep_rg)} row-groups  →  ~{cum/1024**3:.2f} GB\")\n",
    "\n",
    "# # read & write subset\n",
    "# tables = [pq_src.read_row_group(i) for i in keep_rg]\n",
    "# subset = pa.concat_tables(tables)\n",
    "# pq.write_table(subset, DST, compression=\"snappy\")   # or \"zstd\"\n",
    "\n",
    "# print(f\"Subset rows: {subset.num_rows:,}\")\n",
    "# print(\"Wrote:\", DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38dfe004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in file: 16895213\n",
      "Columns in file: 193\n",
      "Schema: <pyarrow._parquet.ParquetSchema object at 0x112aa7ac0>\n",
      "required group field_id=-1 duckdb_schema {\n",
      "  optional fixed_len_byte_array(16) field_id=-1 line_ID (UUID);\n",
      "  optional binary field_id=-1 customer_ID (String);\n",
      "  optional int64 field_id=-1 date (Timestamp(isAdjustedToUTC=false, timeUnit=microseconds, is_from_converted_type=false, force_set_converted_type=false));\n",
      "  optional float field_id=-1 P_2;\n",
      "  optional float field_id=-1 D_39;\n",
      "  optional float field_id=-1 B_1;\n",
      "  optional float field_id=-1 B_2;\n",
      "  optional float field_id=-1 R_1;\n",
      "  optional float field_id=-1 S_3;\n",
      "  optional float field_id=-1 D_41;\n",
      "  optional float field_id=-1 B_3;\n",
      "  optional float field_id=-1 D_42;\n",
      "  optional float field_id=-1 D_43;\n",
      "  optional float field_id=-1 D_44;\n",
      "  optional float field_id=-1 B_4;\n",
      "  optional float field_id=-1 D_45;\n",
      "  optional float field_id=-1 B_5;\n",
      "  optional float field_id=-1 R_2;\n",
      "  optional float field_id=-1 D_46;\n",
      "  optional float field_id=-1 D_47;\n",
      "  optional float field_id=-1 D_48;\n",
      "  optional float field_id=-1 D_49;\n",
      "  optional float field_id=-1 B_6;\n",
      "  optional float field_id=-1 B_7;\n",
      "  optional float field_id=-1 B_8;\n",
      "  optional float field_id=-1 D_50;\n",
      "  optional float field_id=-1 D_51;\n",
      "  optional float field_id=-1 B_9;\n",
      "  optional float field_id=-1 R_3;\n",
      "  optional float field_id=-1 D_52;\n",
      "  optional float field_id=-1 P_3;\n",
      "  optional float field_id=-1 B_10;\n",
      "  optional float field_id=-1 D_53;\n",
      "  optional float field_id=-1 S_5;\n",
      "  optional float field_id=-1 B_11;\n",
      "  optional float field_id=-1 S_6;\n",
      "  optional float field_id=-1 D_54;\n",
      "  optional float field_id=-1 R_4;\n",
      "  optional float field_id=-1 S_7;\n",
      "  optional float field_id=-1 B_12;\n",
      "  optional float field_id=-1 S_8;\n",
      "  optional float field_id=-1 D_55;\n",
      "  optional float field_id=-1 D_56;\n",
      "  optional float field_id=-1 B_13;\n",
      "  optional float field_id=-1 R_5;\n",
      "  optional float field_id=-1 D_58;\n",
      "  optional float field_id=-1 S_9;\n",
      "  optional float field_id=-1 B_14;\n",
      "  optional float field_id=-1 D_59;\n",
      "  optional float field_id=-1 D_60;\n",
      "  optional float field_id=-1 D_61;\n",
      "  optional float field_id=-1 B_15;\n",
      "  optional float field_id=-1 S_11;\n",
      "  optional float field_id=-1 D_62;\n",
      "  optional binary field_id=-1 D_63 (String);\n",
      "  optional binary field_id=-1 D_64 (String);\n",
      "  optional float field_id=-1 D_65;\n",
      "  optional float field_id=-1 B_16;\n",
      "  optional float field_id=-1 B_17;\n",
      "  optional float field_id=-1 B_18;\n",
      "  optional float field_id=-1 B_19;\n",
      "  optional float field_id=-1 D_66;\n",
      "  optional float field_id=-1 B_20;\n",
      "  optional float field_id=-1 D_68;\n",
      "  optional float field_id=-1 S_12;\n",
      "  optional float field_id=-1 R_6;\n",
      "  optional float field_id=-1 S_13;\n",
      "  optional float field_id=-1 B_21;\n",
      "  optional float field_id=-1 D_69;\n",
      "  optional float field_id=-1 B_22;\n",
      "  optional float field_id=-1 D_70;\n",
      "  optional float field_id=-1 D_71;\n",
      "  optional float field_id=-1 D_72;\n",
      "  optional float field_id=-1 S_15;\n",
      "  optional float field_id=-1 B_23;\n",
      "  optional float field_id=-1 D_73;\n",
      "  optional float field_id=-1 P_4;\n",
      "  optional float field_id=-1 D_74;\n",
      "  optional float field_id=-1 D_75;\n",
      "  optional float field_id=-1 D_76;\n",
      "  optional float field_id=-1 B_24;\n",
      "  optional float field_id=-1 R_7;\n",
      "  optional float field_id=-1 D_77;\n",
      "  optional float field_id=-1 B_25;\n",
      "  optional float field_id=-1 B_26;\n",
      "  optional float field_id=-1 D_78;\n",
      "  optional float field_id=-1 D_79;\n",
      "  optional float field_id=-1 R_8;\n",
      "  optional float field_id=-1 R_9;\n",
      "  optional float field_id=-1 S_16;\n",
      "  optional float field_id=-1 D_80;\n",
      "  optional float field_id=-1 R_10;\n",
      "  optional float field_id=-1 R_11;\n",
      "  optional float field_id=-1 B_27;\n",
      "  optional float field_id=-1 D_81;\n",
      "  optional float field_id=-1 D_82;\n",
      "  optional float field_id=-1 S_17;\n",
      "  optional float field_id=-1 R_12;\n",
      "  optional float field_id=-1 B_28;\n",
      "  optional float field_id=-1 R_13;\n",
      "  optional float field_id=-1 D_83;\n",
      "  optional float field_id=-1 R_14;\n",
      "  optional float field_id=-1 R_15;\n",
      "  optional float field_id=-1 D_84;\n",
      "  optional float field_id=-1 R_16;\n",
      "  optional float field_id=-1 B_29;\n",
      "  optional float field_id=-1 B_30;\n",
      "  optional float field_id=-1 S_18;\n",
      "  optional float field_id=-1 D_86;\n",
      "  optional float field_id=-1 D_87;\n",
      "  optional float field_id=-1 R_17;\n",
      "  optional float field_id=-1 R_18;\n",
      "  optional float field_id=-1 D_88;\n",
      "  optional int64 field_id=-1 B_31 (Int(bitWidth=64, isSigned=true));\n",
      "  optional float field_id=-1 S_19;\n",
      "  optional float field_id=-1 R_19;\n",
      "  optional float field_id=-1 B_32;\n",
      "  optional float field_id=-1 S_20;\n",
      "  optional float field_id=-1 R_20;\n",
      "  optional float field_id=-1 R_21;\n",
      "  optional float field_id=-1 B_33;\n",
      "  optional float field_id=-1 D_89;\n",
      "  optional float field_id=-1 R_22;\n",
      "  optional float field_id=-1 R_23;\n",
      "  optional float field_id=-1 D_91;\n",
      "  optional float field_id=-1 D_92;\n",
      "  optional float field_id=-1 D_93;\n",
      "  optional float field_id=-1 D_94;\n",
      "  optional float field_id=-1 R_24;\n",
      "  optional float field_id=-1 R_25;\n",
      "  optional float field_id=-1 D_96;\n",
      "  optional float field_id=-1 S_22;\n",
      "  optional float field_id=-1 S_23;\n",
      "  optional float field_id=-1 S_24;\n",
      "  optional float field_id=-1 S_25;\n",
      "  optional float field_id=-1 S_26;\n",
      "  optional float field_id=-1 D_102;\n",
      "  optional float field_id=-1 D_103;\n",
      "  optional float field_id=-1 D_104;\n",
      "  optional float field_id=-1 D_105;\n",
      "  optional float field_id=-1 D_106;\n",
      "  optional float field_id=-1 D_107;\n",
      "  optional float field_id=-1 B_36;\n",
      "  optional float field_id=-1 B_37;\n",
      "  optional float field_id=-1 R_26;\n",
      "  optional float field_id=-1 R_27;\n",
      "  optional float field_id=-1 B_38;\n",
      "  optional float field_id=-1 D_108;\n",
      "  optional float field_id=-1 D_109;\n",
      "  optional float field_id=-1 D_110;\n",
      "  optional float field_id=-1 D_111;\n",
      "  optional float field_id=-1 B_39;\n",
      "  optional float field_id=-1 D_112;\n",
      "  optional float field_id=-1 B_40;\n",
      "  optional float field_id=-1 S_27;\n",
      "  optional float field_id=-1 D_113;\n",
      "  optional float field_id=-1 D_114;\n",
      "  optional float field_id=-1 D_115;\n",
      "  optional float field_id=-1 D_116;\n",
      "  optional float field_id=-1 D_117;\n",
      "  optional float field_id=-1 D_118;\n",
      "  optional float field_id=-1 D_119;\n",
      "  optional float field_id=-1 D_120;\n",
      "  optional float field_id=-1 D_121;\n",
      "  optional float field_id=-1 D_122;\n",
      "  optional float field_id=-1 D_123;\n",
      "  optional float field_id=-1 D_124;\n",
      "  optional float field_id=-1 D_125;\n",
      "  optional float field_id=-1 D_126;\n",
      "  optional float field_id=-1 D_127;\n",
      "  optional float field_id=-1 D_128;\n",
      "  optional float field_id=-1 D_129;\n",
      "  optional float field_id=-1 B_41;\n",
      "  optional float field_id=-1 B_42;\n",
      "  optional float field_id=-1 D_130;\n",
      "  optional float field_id=-1 D_131;\n",
      "  optional float field_id=-1 D_132;\n",
      "  optional float field_id=-1 D_133;\n",
      "  optional float field_id=-1 R_28;\n",
      "  optional float field_id=-1 D_134;\n",
      "  optional float field_id=-1 D_135;\n",
      "  optional float field_id=-1 D_136;\n",
      "  optional float field_id=-1 D_137;\n",
      "  optional float field_id=-1 D_138;\n",
      "  optional float field_id=-1 D_139;\n",
      "  optional float field_id=-1 D_140;\n",
      "  optional float field_id=-1 D_141;\n",
      "  optional float field_id=-1 D_142;\n",
      "  optional float field_id=-1 D_143;\n",
      "  optional float field_id=-1 D_144;\n",
      "  optional float field_id=-1 D_145;\n",
      "  optional int32 field_id=-1 target (Int(bitWidth=8, isSigned=true));\n",
      "  optional int32 field_id=-1 test (Int(bitWidth=8, isSigned=true));\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "#sanity check of parquet file\n",
    "pq_file = pq.ParquetFile(\"data.parquet\")\n",
    "print(\"Rows in file:\", pq_file.metadata.num_rows)\n",
    "print(\"Columns in file:\", pq_file.metadata.num_columns)\n",
    "print(\"Schema:\", pq_file.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e9b40",
   "metadata": {},
   "source": [
    "#### Converting Parquet file to CSV and HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import time, datetime, os, psutil\n",
    "import xgboost as xgb\n",
    "from pathlib import Path, PureWindowsPath\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc8fc3",
   "metadata": {},
   "source": [
    "Below is converting the initial ~11gb parquet file to CSV. To do this, I set the source (initial parquet file) and the destination path, and opening the parquet to discover how many row-groups to iterate. Then the loop reads one row-group at a time to keep memory low so the entire process doesn't explode in memory, then converting the columnar arrow buffers into a pandas dataframe, appending that chunk to the CSV (writing the header only once), then freeing the RAM before loading the next chunk. Throughout this process I monitored the conversion, and although I forgot to code the time it took ~25 min to convert ending in ~32 gb CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a30a1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row groups in file: 169\n",
      "row-group 1/169 appended\n",
      "row-group 2/169 appended\n",
      "row-group 3/169 appended\n",
      "row-group 4/169 appended\n",
      "row-group 5/169 appended\n",
      "row-group 6/169 appended\n",
      "row-group 7/169 appended\n",
      "row-group 8/169 appended\n",
      "row-group 9/169 appended\n",
      "row-group 10/169 appended\n",
      "row-group 11/169 appended\n",
      "row-group 12/169 appended\n",
      "row-group 13/169 appended\n",
      "row-group 14/169 appended\n",
      "row-group 15/169 appended\n",
      "row-group 16/169 appended\n",
      "row-group 17/169 appended\n",
      "row-group 18/169 appended\n",
      "row-group 19/169 appended\n",
      "row-group 20/169 appended\n",
      "row-group 21/169 appended\n",
      "row-group 22/169 appended\n",
      "row-group 23/169 appended\n",
      "row-group 24/169 appended\n",
      "row-group 25/169 appended\n",
      "row-group 26/169 appended\n",
      "row-group 27/169 appended\n",
      "row-group 28/169 appended\n",
      "row-group 29/169 appended\n",
      "row-group 30/169 appended\n",
      "row-group 31/169 appended\n",
      "row-group 32/169 appended\n",
      "row-group 33/169 appended\n",
      "row-group 34/169 appended\n",
      "row-group 35/169 appended\n",
      "row-group 36/169 appended\n",
      "row-group 37/169 appended\n",
      "row-group 38/169 appended\n",
      "row-group 39/169 appended\n",
      "row-group 40/169 appended\n",
      "row-group 41/169 appended\n",
      "row-group 42/169 appended\n",
      "row-group 43/169 appended\n",
      "row-group 44/169 appended\n",
      "row-group 45/169 appended\n",
      "row-group 46/169 appended\n",
      "row-group 47/169 appended\n",
      "row-group 48/169 appended\n",
      "row-group 49/169 appended\n",
      "row-group 50/169 appended\n",
      "row-group 51/169 appended\n",
      "row-group 52/169 appended\n",
      "row-group 53/169 appended\n",
      "row-group 54/169 appended\n",
      "row-group 55/169 appended\n",
      "row-group 56/169 appended\n",
      "row-group 57/169 appended\n",
      "row-group 58/169 appended\n",
      "row-group 59/169 appended\n",
      "row-group 60/169 appended\n",
      "row-group 61/169 appended\n",
      "row-group 62/169 appended\n",
      "row-group 63/169 appended\n",
      "row-group 64/169 appended\n",
      "row-group 65/169 appended\n",
      "row-group 66/169 appended\n",
      "row-group 67/169 appended\n",
      "row-group 68/169 appended\n",
      "row-group 69/169 appended\n",
      "row-group 70/169 appended\n",
      "row-group 71/169 appended\n",
      "row-group 72/169 appended\n",
      "row-group 73/169 appended\n",
      "row-group 74/169 appended\n",
      "row-group 75/169 appended\n",
      "row-group 76/169 appended\n",
      "row-group 77/169 appended\n",
      "row-group 78/169 appended\n",
      "row-group 79/169 appended\n",
      "row-group 80/169 appended\n",
      "row-group 81/169 appended\n",
      "row-group 82/169 appended\n",
      "row-group 83/169 appended\n",
      "row-group 84/169 appended\n",
      "row-group 85/169 appended\n",
      "row-group 86/169 appended\n",
      "row-group 87/169 appended\n",
      "row-group 88/169 appended\n",
      "row-group 89/169 appended\n",
      "row-group 90/169 appended\n",
      "row-group 91/169 appended\n",
      "row-group 92/169 appended\n",
      "row-group 93/169 appended\n",
      "row-group 94/169 appended\n",
      "row-group 95/169 appended\n",
      "row-group 96/169 appended\n",
      "row-group 97/169 appended\n",
      "row-group 98/169 appended\n",
      "row-group 99/169 appended\n",
      "row-group 100/169 appended\n",
      "row-group 101/169 appended\n",
      "row-group 102/169 appended\n",
      "row-group 103/169 appended\n",
      "row-group 104/169 appended\n",
      "row-group 105/169 appended\n",
      "row-group 106/169 appended\n",
      "row-group 107/169 appended\n",
      "row-group 108/169 appended\n",
      "row-group 109/169 appended\n",
      "row-group 110/169 appended\n",
      "row-group 111/169 appended\n",
      "row-group 112/169 appended\n",
      "row-group 113/169 appended\n",
      "row-group 114/169 appended\n",
      "row-group 115/169 appended\n",
      "row-group 116/169 appended\n",
      "row-group 117/169 appended\n",
      "row-group 118/169 appended\n",
      "row-group 119/169 appended\n",
      "row-group 120/169 appended\n",
      "row-group 121/169 appended\n",
      "row-group 122/169 appended\n",
      "row-group 123/169 appended\n",
      "row-group 124/169 appended\n",
      "row-group 125/169 appended\n",
      "row-group 126/169 appended\n",
      "row-group 127/169 appended\n",
      "row-group 128/169 appended\n",
      "row-group 129/169 appended\n",
      "row-group 130/169 appended\n",
      "row-group 131/169 appended\n",
      "row-group 132/169 appended\n",
      "row-group 133/169 appended\n",
      "row-group 134/169 appended\n",
      "row-group 135/169 appended\n",
      "row-group 136/169 appended\n",
      "row-group 137/169 appended\n",
      "row-group 138/169 appended\n",
      "row-group 139/169 appended\n",
      "row-group 140/169 appended\n",
      "row-group 141/169 appended\n",
      "row-group 142/169 appended\n",
      "row-group 143/169 appended\n",
      "row-group 144/169 appended\n",
      "row-group 145/169 appended\n",
      "row-group 146/169 appended\n",
      "row-group 147/169 appended\n",
      "row-group 148/169 appended\n",
      "row-group 149/169 appended\n",
      "row-group 150/169 appended\n",
      "row-group 151/169 appended\n",
      "row-group 152/169 appended\n",
      "row-group 153/169 appended\n",
      "row-group 154/169 appended\n",
      "row-group 155/169 appended\n",
      "row-group 156/169 appended\n",
      "row-group 157/169 appended\n",
      "row-group 158/169 appended\n",
      "row-group 159/169 appended\n",
      "row-group 160/169 appended\n",
      "row-group 161/169 appended\n",
      "row-group 162/169 appended\n",
      "row-group 163/169 appended\n",
      "row-group 164/169 appended\n",
      "row-group 165/169 appended\n",
      "row-group 166/169 appended\n",
      "row-group 167/169 appended\n",
      "row-group 168/169 appended\n",
      "row-group 169/169 appended\n",
      "All done -> data.csv\n"
     ]
    }
   ],
   "source": [
    "# in/out file paths\n",
    "IN_FILE = Path(\"data.parquet\")\n",
    "OUT_CSV = Path(\"data.csv\") \n",
    "#opening the parquet file\n",
    "pq_file = pq.ParquetFile(IN_FILE, memory_map=True) #zero copy i/o data\n",
    "num_row_groups = pq_file.num_row_groups #number of row groups, meaning the partitions on disk\n",
    "print(f\"Row groups in file: {num_row_groups}\")\n",
    "\n",
    "# write loop\n",
    "first_chunk = True #controlling the header write\n",
    "for row_group in range(num_row_groups):\n",
    "    # load one row group into an arrow table to stay off heap but still columnar\n",
    "    table = pq_file.read_row_group(row_group)\n",
    "    # converting to pandas, the arrowDtype preserves the nullable types during the conversion\n",
    "    df = table.to_pandas(types_mapper=pd.ArrowDtype)\n",
    "    # writing / appending\n",
    "    if first_chunk: #create/overwrite file and write to header row\n",
    "        df.to_csv(OUT_CSV, index=False, mode=\"w\", header=True)\n",
    "        first_chunk = False\n",
    "    else: #append rows and skip header\n",
    "        df.to_csv(OUT_CSV, index=False, mode=\"a\", header=False)\n",
    "    \n",
    "    # free memory for chunk before next iteration\n",
    "    del df, table\n",
    "    gc.collect()\n",
    "    print(f\"row-group {row_group+1}/{num_row_groups} appended\")\n",
    "\n",
    "print(\"All done ->\", OUT_CSV) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fd0c8",
   "metadata": {},
   "source": [
    "Before converting to HDF5, below is a sanity check that there wasn't any loss or misrepresentation in data between the files. The only visible difference is the date display; Parquet keeps it as pandas datetime64[ns], while the csv reader shows a string timestamp of object dtype. we can cast this later but data is still preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66e6f3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             line_ID  \\\n",
      "0  b'\\xb6a\\x82\\x86f#F\\x1d\\x8c\\x94\\x7f\\x8d\\x944\\xd...   \n",
      "1        b'L\\xa8+-\\xa8\\x8dM\\xa9\\x96g\\xed0I\\x95\\x1e$'   \n",
      "2        b']s_\\x87\\xaf B\\xec\\xbeEg\\xb5\\x1e\\xb2\\xaed'   \n",
      "3     b'\\xfb^\\xd4{Q\\xb5HO\\xa8\\xb6\\xf6\\xca\\xb1]@\\x99'   \n",
      "4  b'`\\xa5\\x96\\xf6\\x1b\\rG\\x8d\\xab\\\\\\x16\\x8d\\xe1\\x...   \n",
      "\n",
      "                                         customer_ID       date       P_2  \\\n",
      "0  d00b98b2401d26197fa1d6102cdc1c9bbed7c066b8aaa9... 2018-03-06  0.366254   \n",
      "1  d00bc5e66e3aac9eae7c9e94621b36d196566d61ef7a32... 2018-03-25  0.312623   \n",
      "2  d00bd125cf6fa463a6c57b9959b8a4197f6f79fb154fee... 2018-03-28  0.395606   \n",
      "3  d00bfbdee3081206258a4b4fb2ef2eb311697f37056bfb... 2018-03-01  0.977543   \n",
      "4  d00c0dd295ada176c4e697d4cc1cd2f0d572870f770859... 2018-03-26  0.934237   \n",
      "\n",
      "       D_39       B_1       B_2       R_1       S_3      D_41  ...     D_138  \\\n",
      "0  0.003860  0.009151  0.818901  0.008979  0.143153  0.005497  ...  0.500092   \n",
      "1  0.179014  0.560108  0.029272  0.756391  0.091940  0.005489  ...       NaN   \n",
      "2  1.066026  0.731072  0.019496  0.751631  0.728473  0.574862  ...       NaN   \n",
      "3  0.299848  0.016523  1.002691  0.009632  0.107092  0.003081  ...       NaN   \n",
      "4  0.003405  0.295735  1.001624  0.005357  0.142128  0.000591  ...       NaN   \n",
      "\n",
      "      D_139     D_140     D_141     D_142     D_143     D_144     D_145  \\\n",
      "0  0.004264  0.009833  0.000985       NaN  0.006703  0.005501  0.005352   \n",
      "1  0.005151  0.005813  0.007565       NaN  0.000990  0.001063  0.008251   \n",
      "2  1.008811  0.001852  0.964900  0.478015  1.001196  0.173963  0.091996   \n",
      "3  0.006636  0.003483  0.005275       NaN  0.009845  0.009467  0.008976   \n",
      "4  1.004110  0.002928  1.086800  1.076618  1.001960  1.003030  0.097089   \n",
      "\n",
      "   target  test  \n",
      "0     0.0     0  \n",
      "1     1.0     0  \n",
      "2     1.0     0  \n",
      "3     0.0     0  \n",
      "4     0.0     0  \n",
      "\n",
      "[5 rows x 193 columns]\n",
      "-------\n",
      "-------\n",
      "-------\n",
      "-------\n",
      "-------\n",
      "                                             line_ID  \\\n",
      "0  b'\\xb6a\\x82\\x86f#F\\x1d\\x8c\\x94\\x7f\\x8d\\x944\\xd...   \n",
      "1        b'L\\xa8+-\\xa8\\x8dM\\xa9\\x96g\\xed0I\\x95\\x1e$'   \n",
      "2        b']s_\\x87\\xaf B\\xec\\xbeEg\\xb5\\x1e\\xb2\\xaed'   \n",
      "3     b'\\xfb^\\xd4{Q\\xb5HO\\xa8\\xb6\\xf6\\xca\\xb1]@\\x99'   \n",
      "4  b'`\\xa5\\x96\\xf6\\x1b\\rG\\x8d\\xab\\\\\\x16\\x8d\\xe1\\x...   \n",
      "\n",
      "                                         customer_ID                 date  \\\n",
      "0  d00b98b2401d26197fa1d6102cdc1c9bbed7c066b8aaa9...  2018-03-06 00:00:00   \n",
      "1  d00bc5e66e3aac9eae7c9e94621b36d196566d61ef7a32...  2018-03-25 00:00:00   \n",
      "2  d00bd125cf6fa463a6c57b9959b8a4197f6f79fb154fee...  2018-03-28 00:00:00   \n",
      "3  d00bfbdee3081206258a4b4fb2ef2eb311697f37056bfb...  2018-03-01 00:00:00   \n",
      "4  d00c0dd295ada176c4e697d4cc1cd2f0d572870f770859...  2018-03-26 00:00:00   \n",
      "\n",
      "        P_2      D_39       B_1       B_2       R_1       S_3      D_41  ...  \\\n",
      "0  0.366254  0.003860  0.009151  0.818901  0.008979  0.143153  0.005497  ...   \n",
      "1  0.312623  0.179014  0.560108  0.029272  0.756391  0.091940  0.005489  ...   \n",
      "2  0.395606  1.066026  0.731072  0.019496  0.751631  0.728473  0.574862  ...   \n",
      "3  0.977543  0.299848  0.016523  1.002691  0.009632  0.107092  0.003081  ...   \n",
      "4  0.934237  0.003405  0.295735  1.001624  0.005357  0.142128  0.000591  ...   \n",
      "\n",
      "      D_138     D_139     D_140     D_141     D_142     D_143     D_144  \\\n",
      "0  0.500092  0.004264  0.009833  0.000985       NaN  0.006703  0.005501   \n",
      "1       NaN  0.005151  0.005813  0.007565       NaN  0.000990  0.001063   \n",
      "2       NaN  1.008811  0.001852  0.964900  0.478015  1.001196  0.173963   \n",
      "3       NaN  0.006636  0.003483  0.005275       NaN  0.009845  0.009467   \n",
      "4       NaN  1.004110  0.002928  1.086800  1.076618  1.001960  1.003030   \n",
      "\n",
      "      D_145  target  test  \n",
      "0  0.005352       0     0  \n",
      "1  0.008251       1     0  \n",
      "2  0.091996       1     0  \n",
      "3  0.008976       0     0  \n",
      "4  0.097089       0     0  \n",
      "\n",
      "[5 rows x 193 columns]\n",
      "row lengths are the same: True\n"
     ]
    }
   ],
   "source": [
    "#verifying both files before hdf5 conversion\n",
    "df_pq = pq.read_table(\"data.parquet\").to_pandas().head(5)\n",
    "df_csv = pd.read_csv(\"data.csv\", nrows=5)\n",
    "\n",
    "print(df_pq.head())\n",
    "print(\"-------\\n-------\\n-------\\n-------\\n-------\")\n",
    "print(df_csv.head())\n",
    "\n",
    "print(\"row lengths are the same:\", len(df_pq) == len(df_csv)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6434c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy : 1.24.4\n",
      "PyTables : 3.9.2\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tables\n",
    "print(\"NumPy :\", numpy.__version__)\n",
    "print(\"PyTables :\", tables.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c978e",
   "metadata": {},
   "source": [
    "Converting Parquet -> HDF5: \n",
    "\n",
    "Earlier for parquet -> CSV, we streamed a single row group at a time ,turned the colkumnar buffer into a pandas DF, and then appended the slice to the CSV, and since CSV is just UTF-8 text, it carried no compression or type information. This new script serves a different purpose, we're streaming row-groups but our goal is a binary file that reloads quickly while staying compact on the disk. Here we're appending the data to a HDF5 table encoded with the \"Blosc-zstd\" codec at level 6 which compresses on every CPU core so each chunk is written extremely quickly. We batched 4 row-groups before every append call which halves the PyTable's metadata overhead, and casted the line_ID column to a fixed-width S16 byte-type to save time and space. Then we forced the target/test columns to used signed Int8 and fill missing values with -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58c7494d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE row-group 1/169  |  elapsed 0.0 min\n",
      "DONE row-group 2/169  |  elapsed 0.0 min\n",
      "DONE row-group 3/169  |  elapsed 0.0 min\n",
      "DONE row-group 4/169  |  elapsed 0.0 min\n",
      "DONE row-group 5/169  |  elapsed 0.0 min\n",
      "DONE row-group 6/169  |  elapsed 0.0 min\n",
      "DONE row-group 7/169  |  elapsed 0.0 min\n",
      "DONE row-group 8/169  |  elapsed 1.9 min\n",
      "DONE row-group 9/169  |  elapsed 1.9 min\n",
      "DONE row-group 10/169  |  elapsed 1.9 min\n",
      "DONE row-group 11/169  |  elapsed 1.9 min\n",
      "DONE row-group 12/169  |  elapsed 3.7 min\n",
      "DONE row-group 13/169  |  elapsed 3.7 min\n",
      "DONE row-group 14/169  |  elapsed 3.7 min\n",
      "DONE row-group 15/169  |  elapsed 3.7 min\n",
      "DONE row-group 16/169  |  elapsed 5.6 min\n",
      "DONE row-group 17/169  |  elapsed 5.6 min\n",
      "DONE row-group 18/169  |  elapsed 5.6 min\n",
      "DONE row-group 19/169  |  elapsed 5.6 min\n",
      "DONE row-group 20/169  |  elapsed 7.6 min\n",
      "DONE row-group 21/169  |  elapsed 7.6 min\n",
      "DONE row-group 22/169  |  elapsed 7.6 min\n",
      "DONE row-group 23/169  |  elapsed 7.6 min\n",
      "DONE row-group 24/169  |  elapsed 9.6 min\n",
      "DONE row-group 25/169  |  elapsed 9.6 min\n",
      "DONE row-group 26/169  |  elapsed 9.6 min\n",
      "DONE row-group 27/169  |  elapsed 9.6 min\n",
      "DONE row-group 28/169  |  elapsed 11.7 min\n",
      "DONE row-group 29/169  |  elapsed 11.7 min\n",
      "DONE row-group 30/169  |  elapsed 11.7 min\n",
      "DONE row-group 31/169  |  elapsed 11.7 min\n",
      "DONE row-group 32/169  |  elapsed 13.6 min\n",
      "DONE row-group 33/169  |  elapsed 13.6 min\n",
      "DONE row-group 34/169  |  elapsed 13.6 min\n",
      "DONE row-group 35/169  |  elapsed 13.6 min\n",
      "DONE row-group 36/169  |  elapsed 15.5 min\n",
      "DONE row-group 37/169  |  elapsed 15.5 min\n",
      "DONE row-group 38/169  |  elapsed 15.5 min\n",
      "DONE row-group 39/169  |  elapsed 15.5 min\n",
      "DONE row-group 40/169  |  elapsed 17.4 min\n",
      "DONE row-group 41/169  |  elapsed 17.4 min\n",
      "DONE row-group 42/169  |  elapsed 17.4 min\n",
      "DONE row-group 43/169  |  elapsed 17.4 min\n",
      "DONE row-group 44/169  |  elapsed 19.3 min\n",
      "DONE row-group 45/169  |  elapsed 19.3 min\n",
      "DONE row-group 46/169  |  elapsed 19.3 min\n",
      "DONE row-group 47/169  |  elapsed 19.3 min\n",
      "DONE row-group 48/169  |  elapsed 21.2 min\n",
      "DONE row-group 49/169  |  elapsed 21.2 min\n",
      "DONE row-group 50/169  |  elapsed 21.2 min\n",
      "DONE row-group 51/169  |  elapsed 21.2 min\n",
      "DONE row-group 52/169  |  elapsed 27.9 min\n",
      "DONE row-group 53/169  |  elapsed 27.9 min\n",
      "DONE row-group 54/169  |  elapsed 27.9 min\n",
      "DONE row-group 55/169  |  elapsed 27.9 min\n",
      "DONE row-group 56/169  |  elapsed 29.7 min\n",
      "DONE row-group 57/169  |  elapsed 29.7 min\n",
      "DONE row-group 58/169  |  elapsed 29.7 min\n",
      "DONE row-group 59/169  |  elapsed 29.8 min\n",
      "DONE row-group 60/169  |  elapsed 34.9 min\n",
      "DONE row-group 61/169  |  elapsed 34.9 min\n",
      "DONE row-group 62/169  |  elapsed 34.9 min\n",
      "DONE row-group 63/169  |  elapsed 34.9 min\n",
      "DONE row-group 64/169  |  elapsed 36.8 min\n",
      "DONE row-group 65/169  |  elapsed 36.8 min\n",
      "DONE row-group 66/169  |  elapsed 36.8 min\n",
      "DONE row-group 67/169  |  elapsed 36.8 min\n",
      "DONE row-group 68/169  |  elapsed 39.2 min\n",
      "DONE row-group 69/169  |  elapsed 39.2 min\n",
      "DONE row-group 70/169  |  elapsed 39.2 min\n",
      "DONE row-group 71/169  |  elapsed 39.2 min\n",
      "DONE row-group 72/169  |  elapsed 41.3 min\n",
      "DONE row-group 73/169  |  elapsed 41.3 min\n",
      "DONE row-group 74/169  |  elapsed 41.3 min\n",
      "DONE row-group 75/169  |  elapsed 41.3 min\n",
      "DONE row-group 76/169  |  elapsed 43.4 min\n",
      "DONE row-group 77/169  |  elapsed 43.4 min\n",
      "DONE row-group 78/169  |  elapsed 43.4 min\n",
      "DONE row-group 79/169  |  elapsed 43.4 min\n",
      "DONE row-group 80/169  |  elapsed 45.4 min\n",
      "DONE row-group 81/169  |  elapsed 45.4 min\n",
      "DONE row-group 82/169  |  elapsed 45.4 min\n",
      "DONE row-group 83/169  |  elapsed 45.4 min\n",
      "DONE row-group 84/169  |  elapsed 47.4 min\n",
      "DONE row-group 85/169  |  elapsed 47.4 min\n",
      "DONE row-group 86/169  |  elapsed 47.4 min\n",
      "DONE row-group 87/169  |  elapsed 47.4 min\n",
      "DONE row-group 88/169  |  elapsed 49.4 min\n",
      "DONE row-group 89/169  |  elapsed 49.4 min\n",
      "DONE row-group 90/169  |  elapsed 49.4 min\n",
      "DONE row-group 91/169  |  elapsed 49.4 min\n",
      "DONE row-group 92/169  |  elapsed 51.3 min\n",
      "DONE row-group 93/169  |  elapsed 51.3 min\n",
      "DONE row-group 94/169  |  elapsed 51.3 min\n",
      "DONE row-group 95/169  |  elapsed 51.3 min\n",
      "DONE row-group 96/169  |  elapsed 53.2 min\n",
      "DONE row-group 97/169  |  elapsed 53.2 min\n",
      "DONE row-group 98/169  |  elapsed 53.2 min\n",
      "DONE row-group 99/169  |  elapsed 53.2 min\n",
      "DONE row-group 100/169  |  elapsed 55.2 min\n",
      "DONE row-group 101/169  |  elapsed 55.2 min\n",
      "DONE row-group 102/169  |  elapsed 55.2 min\n",
      "DONE row-group 103/169  |  elapsed 55.2 min\n",
      "DONE row-group 104/169  |  elapsed 57.8 min\n",
      "DONE row-group 105/169  |  elapsed 57.8 min\n",
      "DONE row-group 106/169  |  elapsed 57.8 min\n",
      "DONE row-group 107/169  |  elapsed 57.8 min\n",
      "DONE row-group 108/169  |  elapsed 59.7 min\n",
      "DONE row-group 109/169  |  elapsed 59.7 min\n",
      "DONE row-group 110/169  |  elapsed 59.7 min\n",
      "DONE row-group 111/169  |  elapsed 59.7 min\n",
      "DONE row-group 112/169  |  elapsed 61.6 min\n",
      "DONE row-group 113/169  |  elapsed 61.6 min\n",
      "DONE row-group 114/169  |  elapsed 61.6 min\n",
      "DONE row-group 115/169  |  elapsed 61.6 min\n",
      "DONE row-group 116/169  |  elapsed 63.5 min\n",
      "DONE row-group 117/169  |  elapsed 63.5 min\n",
      "DONE row-group 118/169  |  elapsed 63.5 min\n",
      "DONE row-group 119/169  |  elapsed 63.5 min\n",
      "DONE row-group 120/169  |  elapsed 65.4 min\n",
      "DONE row-group 121/169  |  elapsed 65.4 min\n",
      "DONE row-group 122/169  |  elapsed 65.4 min\n",
      "DONE row-group 123/169  |  elapsed 65.4 min\n",
      "DONE row-group 124/169  |  elapsed 69.4 min\n",
      "DONE row-group 125/169  |  elapsed 69.4 min\n",
      "DONE row-group 126/169  |  elapsed 69.4 min\n",
      "DONE row-group 127/169  |  elapsed 69.4 min\n",
      "DONE row-group 128/169  |  elapsed 71.4 min\n",
      "DONE row-group 129/169  |  elapsed 71.4 min\n",
      "DONE row-group 130/169  |  elapsed 71.4 min\n",
      "DONE row-group 131/169  |  elapsed 71.4 min\n",
      "DONE row-group 132/169  |  elapsed 73.3 min\n",
      "DONE row-group 133/169  |  elapsed 73.3 min\n",
      "DONE row-group 134/169  |  elapsed 73.3 min\n",
      "DONE row-group 135/169  |  elapsed 73.3 min\n",
      "DONE row-group 136/169  |  elapsed 75.2 min\n",
      "DONE row-group 137/169  |  elapsed 75.2 min\n",
      "DONE row-group 138/169  |  elapsed 75.2 min\n",
      "DONE row-group 139/169  |  elapsed 75.3 min\n",
      "DONE row-group 140/169  |  elapsed 77.2 min\n",
      "DONE row-group 141/169  |  elapsed 77.2 min\n",
      "DONE row-group 142/169  |  elapsed 77.2 min\n",
      "DONE row-group 143/169  |  elapsed 77.2 min\n",
      "DONE row-group 144/169  |  elapsed 79.1 min\n",
      "DONE row-group 145/169  |  elapsed 79.1 min\n",
      "DONE row-group 146/169  |  elapsed 79.1 min\n",
      "DONE row-group 147/169  |  elapsed 79.1 min\n",
      "DONE row-group 148/169  |  elapsed 81.1 min\n",
      "DONE row-group 149/169  |  elapsed 81.1 min\n",
      "DONE row-group 150/169  |  elapsed 81.1 min\n",
      "DONE row-group 151/169  |  elapsed 81.1 min\n",
      "DONE row-group 152/169  |  elapsed 83.0 min\n",
      "DONE row-group 153/169  |  elapsed 83.0 min\n",
      "DONE row-group 154/169  |  elapsed 83.0 min\n",
      "DONE row-group 155/169  |  elapsed 83.0 min\n",
      "DONE row-group 156/169  |  elapsed 85.0 min\n",
      "DONE row-group 157/169  |  elapsed 85.0 min\n",
      "DONE row-group 158/169  |  elapsed 85.0 min\n",
      "DONE row-group 159/169  |  elapsed 85.0 min\n",
      "DONE row-group 160/169  |  elapsed 86.9 min\n",
      "DONE row-group 161/169  |  elapsed 86.9 min\n",
      "DONE row-group 162/169  |  elapsed 86.9 min\n",
      "DONE row-group 163/169  |  elapsed 86.9 min\n",
      "DONE row-group 164/169  |  elapsed 88.8 min\n",
      "DONE row-group 165/169  |  elapsed 88.8 min\n",
      "DONE row-group 166/169  |  elapsed 88.8 min\n",
      "DONE row-group 167/169  |  elapsed 88.8 min\n",
      "DONE row-group 168/169  |  elapsed 90.8 min\n",
      "DONE row-group 169/169  |  elapsed 92.8 min\n",
      "\n",
      "Parquet → HDF5 completed in 92.8 minutes\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq, pandas as pd, numpy as np, gc, time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tables\n",
    "tables.parameters.MAX_BLOSC_THREADS = os.cpu_count()  # enable Blosc threads to use all cpu cores for decompression\n",
    "#source / destination files\n",
    "IN_PARQUET = Path(\"data.parquet\")\n",
    "OUT_H5 = Path(\"data.h5\")\n",
    "\n",
    "#open parquet file and getting how many row groups we need to stream\n",
    "pq_file = pq.ParquetFile(IN_PARQUET, memory_map=True) # zero copy i/o data\n",
    "num_row_groups = pq_file.num_row_groups\n",
    "\n",
    "#iterating through the row groups and writing to hdf5\n",
    "t0 = time.time()\n",
    "# \"w\" -> create / overwrite HDF5 file\n",
    "# \"blosc:zstd\" -> compression algorithm, its a faster multithreaded codec\n",
    "# \"6\" -> compression level (0-9), 1 being fastest but minimal compression, 9 being slowest and highest compression\n",
    "buf = []\n",
    "with pd.HDFStore(OUT_H5, \"w\", complib=\"blosc:zstd\", complevel=2) as s:\n",
    "    for i in range(num_row_groups):\n",
    "        #loading one row group into pandas df\n",
    "        df = pq_file.read_row_group(i).to_pandas()\n",
    "        #storing column as fixed width bytes \n",
    "        df[\"line_ID\"] = df[\"line_ID\"].apply(bytes.hex)\n",
    "        # normalizing / forcing the label columns to use signed Int8 and use –1 for “missing”\n",
    "        for col in [\"target\", \"test\"]:\n",
    "            df[col] = df[col].fillna(-1).astype(np.int8)\n",
    "        #staging the chunk\n",
    "        buf.append(df)\n",
    "        #when the buffer holds 4 row groups OR its the last row group, write to hdf5\n",
    "        if len(buf) == 4 or i == num_row_groups - 1:\n",
    "            #train being HDF5 table node\n",
    "            s.append(\"train\", df, data_columns=True, index=False) \n",
    "            df = pd.concat(buf, ignore_index=True)\n",
    "            buf.clear() #freeing batch memory\n",
    "        # del df\n",
    "        # gc.collect()\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"DONE row-group {i+1}/{num_row_groups}  |  elapsed {elapsed/60:.1f} min\")\n",
    "\n",
    "#printing the total time taken to convert\n",
    "total = time.time() - t0\n",
    "print(f\"\\nParquet → HDF5 completed in {total/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40dec8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         customer_ID                    date  \\\n",
      "0  2e4ca86c940208da97886282b6f09140ca8896cf99674d... 1970-01-18 13:37:55.200   \n",
      "1  2e4cdb37d30882655633b22a6f59c93dc52fce83730e9d... 1970-01-18 14:03:50.400   \n",
      "2  2e4cf74c31837113e973e20a0a3740996633790899fcbb... 1970-01-18 14:00:57.600   \n",
      "3  2e4d1d2e275c48ca9dadee3fea591d0c5b161d148643c5... 1970-01-18 14:00:57.600   \n",
      "4  2e4d2f149c4c205429b6f5af7f25ae9053cedf48033011... 1970-01-18 13:33:36.000   \n",
      "\n",
      "        P_2      D_39       B_1       B_2       R_1       S_3      D_41  \\\n",
      "0  0.839014  0.005983  0.008021  0.814622  0.008639       NaN  0.005880   \n",
      "1  0.907406  0.003602  0.001457  0.810995  0.000260       NaN  0.009017   \n",
      "2  1.001385  0.002470  0.009373  1.003198  0.003521  0.014935  0.009391   \n",
      "3  0.975932  0.004880  0.008085  0.818724  0.009377  0.129178  0.002112   \n",
      "4  0.695464  0.037073  0.142873  0.032505  0.000986       NaN  0.009178   \n",
      "\n",
      "        B_3  ...  D_138     D_139     D_140     D_141  D_142     D_143  \\\n",
      "0  0.008358  ...    NaN  0.006438  0.006698  0.007678    NaN  0.006619   \n",
      "1  0.000577  ...    NaN  0.004734  0.005858  0.007896    NaN  0.004651   \n",
      "2  0.001991  ...    NaN  0.005232  0.008959  0.001730    NaN  0.006638   \n",
      "3  0.003768  ...    NaN  0.007007  0.004747  0.005726    NaN  0.002102   \n",
      "4  0.420426  ...    NaN  0.006553  0.000784  0.003418    NaN  0.000194   \n",
      "\n",
      "      D_144     D_145  target  test  \n",
      "0  0.008396  0.000536       0     0  \n",
      "1  0.005604  0.007078       0     0  \n",
      "2  0.007813  0.008732       0     0  \n",
      "3  0.005348  0.001577       0     0  \n",
      "4  0.004586  0.005608       1     0  \n",
      "\n",
      "[5 rows x 192 columns]\n",
      "\n",
      "quick read: 0.034 s\n",
      "HDF5 size : 2.58 GB\n"
     ]
    }
   ],
   "source": [
    "# #sanity check of hdf5 file\n",
    "# import pandas as pd, os, time\n",
    "\n",
    "# start = time.time()\n",
    "# df_head = pd.read_hdf(\"data.h5\", key=\"train\", stop=5, decode=False)\n",
    "# print(df_head.head())\n",
    "# print(\"\\nHDF5 quick read time:\", time.time()-start, \"sec\")\n",
    "# print(\"HDF5 size on disk:\", os.path.getsize(\"data.h5\")/1024**3, \"GB\")\n",
    "\n",
    "import pandas as pd, os, time\n",
    "\n",
    "with pd.HDFStore(\"data.h5\", \"r\") as st:\n",
    "    all_cols = st.select(\"train\", stop=0).columns\n",
    "    cols_to_read = [c for c in all_cols if c != \"line_ID\"] # Exclude the problematic column\n",
    "\n",
    "    t0 = time.time()\n",
    "    df_head = st.select(\"train\",\n",
    "                        columns=cols_to_read, # Read only the selected columns\n",
    "                        stop=5)\n",
    "\n",
    "print(df_head) # This should now display without error\n",
    "print(f\"\\nquick read: {time.time()-t0:.3f} s\")\n",
    "print(f\"HDF5 size : {os.path.getsize('data.h5')/1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab27008",
   "metadata": {},
   "source": [
    "##### Benchmarking each file format and its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af7d4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "import csv, os\n",
    "import time\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "#benchmark settings\n",
    "RESULTS_FILE = Path(\"benchmark_results.csv\")  # where we log runs\n",
    "XGB_THREADS = 4 # CPU threads for XGBoost\n",
    "POST_SAMPLE_N = 1_000_000 # sample after load (None -> all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19b4835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting every float64 column to float32 in-place to save RAM\n",
    "def downcast(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    f64 = df.select_dtypes(\"float64\").columns #name of every float64 column \n",
    "    df[f64] = df[f64].astype(np.float32, copy=False) #in-place cast, no copy\n",
    "    return df #returning lighter df\n",
    "\n",
    "#loaders for each file format\n",
    "\n",
    "#csv loader with progress bar\n",
    "def load_csv(path=\"data.csv\", chunksize=250_000, concat_every=8):\n",
    "    total = os.path.getsize(path) #file size in bytes\n",
    "    buf = [] #staging the buffers\n",
    "    big_parts = [] #big parts buffer\n",
    "    n_chunks = 0 #chunk counter\n",
    "    #tqdm shows a live counter of the bytes while reading the text file\n",
    "    with tqdm(total=total, unit=\"B\", unit_scale=True, desc=\"CSV read\") as bar: \n",
    "        # pandas iterator streams 'chunksize' rows at a time\n",
    "        for chunk in pd.read_csv(path, chunksize=chunksize, low_memory=False):\n",
    "            buf.append(downcast(chunk))\n",
    "            n_chunks += 1 #casting and staging chunk\n",
    "            bar.update(chunk.memory_usage(index=False).sum()) #advance bar\n",
    "            # every \"concat_every\" chunks, concatenate and append to big_parts\n",
    "            if n_chunks % concat_every == 0:\n",
    "                big_parts.append(pd.concat(buf, ignore_index=True))\n",
    "                buf = [] #resets small buffer\n",
    "        #flushing leftover chunks\n",
    "        if buf:\n",
    "            big_parts.append(pd.concat(buf, ignore_index=True))\n",
    "    #final df concats N big parts\n",
    "    return pd.concat(big_parts, ignore_index=True)\n",
    "\n",
    "# Parquet loader\n",
    "def load_parquet(path=\"data.parquet\"):\n",
    "    pqf = pq.ParquetFile(path) #open once, zero copy i/o\n",
    "    tables = [] # arrow table buffer\n",
    "    with tqdm(total=pqf.num_row_groups, desc=\"Parquet RG\") as bar: \n",
    "        #iterating through the row groups\n",
    "        for i in range(pqf.num_row_groups):\n",
    "            tables.append(pqf.read_row_group(i)) #read row group -> arrow table\n",
    "            bar.update(1) #progress tick\n",
    "    #concat arrow tables, convert to pandas, downcast floats from float64 -> float32\n",
    "    return downcast(pa.concat_tables(tables).to_pandas())\n",
    "\n",
    "# HDF5 loader (chunked read)\n",
    "def load_hdf5(path=\"data.h5\", chunksize=250_000):\n",
    "    store = pd.HDFStore(path, \"r\") #open HDF5 in read-only mode\n",
    "    nrows = store.get_storer(\"train\").nrows #total rows in tqdm\n",
    "    parts = [] #collected dfs\n",
    "    with tqdm(total=nrows, desc=\"HDF5 rows\") as bar: \n",
    "        # per select, pytables streams a row of size \"chunksize\"\n",
    "        for chunk in store.select(\"train\", chunksize=chunksize): #\n",
    "            parts.append(downcast(chunk))  #casting and staging\n",
    "            bar.update(len(chunk)) # progress rows read\n",
    "    store.close() #close file handle\n",
    "    return pd.concat(parts, ignore_index=True) #stitching chunks together\n",
    "\n",
    "LOADERS = {\"CSV\": load_csv, \"Parquet\": load_parquet, \"HDF5\": load_hdf5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba0f8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the file, sample rows, training XGboost, and logging timings\n",
    "def run_benchmark(fmt: str, path: str):\n",
    "    #measuring ram before and after load phase\n",
    "    proc = psutil.Process() # for RAM measurement\n",
    "    t0 = time.perf_counter() #starting timer\n",
    "    df = LOADERS[fmt](path) # full ingest + float32 cast\n",
    "    load_sec = time.perf_counter() - t0 #elapsed time to parse / load\n",
    "    peak_ram = proc.memory_info().rss / 1024**3 #resident set size (RAM used) in GB\n",
    "\n",
    "    # down-sample so every format trains on equal rows (1M)\n",
    "    if POST_SAMPLE_N and len(df) > POST_SAMPLE_N:\n",
    "        df = df.sample(n=POST_SAMPLE_N, random_state=0)\n",
    "\n",
    "    # building feature matrix and label vector\n",
    "    num_cols = df.select_dtypes(\"number\").columns.drop([\"target\", \"test\"])\n",
    "    X = df[num_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "    if fmt == \"data.h5\":\n",
    "        y = df[\"target\"].clip(lower=0).astype(np.int8, copy=False) #enable for hdf5\n",
    "    else:\n",
    "        y = df[\"target\"].fillna(0).astype(np.int8).to_numpy(copy=False) #enable for csv/parquet\n",
    "\n",
    "    # XGBoost training\n",
    "    dtrain = xgb.DMatrix(X, label=y) \n",
    "    t1 = time.perf_counter()\n",
    "    xgb.train({\"objective\": \"binary:logistic\", #0/1 classification\n",
    "               \"tree_method\": \"hist\", #fast CPU histogram algorithm\n",
    "               \"nthread\": XGB_THREADS}, # limit CPU threads\n",
    "              dtrain, num_boost_round=50, verbose_eval=False) #50 boosting rounds\n",
    "    train_sec = time.perf_counter() - t1 \n",
    "\n",
    "    # log and display results\n",
    "    result = {\"format\": fmt, \"rows\": len(y),\"load_sec\": round(load_sec, 2), \"train_sec\": round(train_sec, 2), \"peak_ram_gb\": round(peak_ram, 2)}\n",
    "    display(result) #dict output \n",
    "\n",
    "    # append row to CSV log\n",
    "    write_header = not RESULTS_FILE.exists() #adding header only once\n",
    "    with RESULTS_FILE.open(\"a\", newline=\"\") as f: #\n",
    "        w = csv.DictWriter(f, fieldnames=result.keys())\n",
    "        if write_header: w.writeheader()\n",
    "        w.writerow(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3cea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CSV read:  41%|████      | 13.5G/33.0G [05:41<08:10, 39.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# running the benchmark on each stored file \n",
    "run_benchmark(\"CSV\", \"data.csv\")\n",
    "run_benchmark(\"Parquet\", \"data.parquet\")\n",
    "run_benchmark(\"HDF5\", \"data.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc922ff",
   "metadata": {},
   "source": [
    "##### Actual benchmarks of each format\n",
    "(See run_benchmark.py, given the nature of how lengthy the process is I had to run each format separately, and asynch)\n",
    "\n",
    "| format  | rows     | load_sec | train_sec | peak_ram_gb |\n",
    "|---------|----------|----------|-----------|-------------|\n",
    "| Parquet | 1000000  | 22.19    | 7.83      | 0.92        |\n",
    "| CSV     | 1000000  | 375.81   | 7.76      | 3.84        |\n",
    "| HDF5    | 1000000  | 96.84    | 7.78      | 7.66        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94a523",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
